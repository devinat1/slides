---
title: A Survey on Large Language Model based Autonomous Agents
author: Devin Ersoy
institute: Purdue University
date: 2024-02-01
format: revealjs
highlight-style: github
slide-number: c/t
---

## The Point
- Model *human thinking process*
- 192 autonomous agents exist
- Create a taxonomy of such efforts by answering the following:
	1. Which architecture should be designed to better use LLMs
	2. How to enable an *agent* to acquire capabilities for accomplishing specific tasks.

:::notes
- The point of an autonomous agent is to model the human thinking process. As of today, 192 different autonomous agents exist. One such familiar project is [AutoGPT](AutoGPT).
- So the point of this paper was to organize those and answer the following questions.
- [Autonomous Agent Table](https://abyssinian-molybdenum-f76.notion.site/237e9f7515d543c0922c74f4c3012a77?v=0a309e53d6454afcbe7a5a7e169be0f9)
:::

## Agent Types
::: {.incremental}
- General Agent: Performs a wide variety of tasks
- Tool Agent: Performs a specific task. Task-specific agent
- Simulation Agent: Simulates a particular environment
- Embodied Agent: Interacts with the physical world
- Game Agent: Plays a particular game
- Web Agent: Interacts with the web
- Assistant Agent: Assists a human
:::

## Agents
![Different Agents](images/Different%20Agents.png)


## Learning Types
- Learn from examples
	- Fine tune from certain data sets
- Learn from feedback
	- ex) VoyagerAI learned from Minecraft environment
- Learn from additional training

## Modules
::: {.incremental}
- Profile Module
	- Who and what an agent is
- Memory Module
	- Split between long term and short term memory
- Planning Module
	- Breaking down tasks into manageable chunks
- Action Module
	- The actual actions taken
:::

## Modules
![AutoGPT Anatomy](images/Pasted%20image%2020240201160149.png)

:::notes
- Profile
	- Context as to who and what an agent *is*
- Memory
	- Split between long term and short term memory, in which long term memory is the data the LLM was trained on, and shorter term memory is what is immediately relevant and reassessed as AutoGPT further prompts itself
- Planning
	- The act of breaking down tasks to manageable chunks
	- Planning with feedback
		- Adaptive approach
			- Iterating based on *previous outcomes*
	- Planning without feedback
		- Rely only on preexisting knowledge
- Action
	- The actual actions taken, such as writing to or executing a file, and the impact of such actions.
:::

## Profile Module
- The initial prompt telling AI what *role* its playing

:::notes
- Similar to what the GPT Builder does within its instructions
:::

## Learning Module
- Learn from examples
	- Fine tune from certain data sets
- Learn from feedback
	- ex) VoyagerAI learned from Minecraft environment
- Learn from additional training

## Alignment Methods
### LLM Generation Method
- Automatic generation of profiles for an LLM

### Dataset Alignment Method
- Create agent profile based custom tailored to real world dataset

## Planning Module
- Initial/"Profile prompt" to create an initial plan/agenda
- Recursively prompt language model with summary description
- Decompose into hour long then to 5 minute chunks the given actions.
- Recursive process of breaking down and rethinking generated by the model

## Types of Planning
- Hand crafted?
- Generated by LLM?
- Database applied?

## Implementation Examples
- AutoGPT
- AgentGPT
- Smallville
- Voyager

## LLM Based Autonomous Agent Evaluation
- How well do the agents work?
- Subjective and Objective criterion:
	- ex) Subjective: How "believable?"
	- ex) Objective: How quickly an axe is created

## Limitations
- Context Window
- Infinite Loop
	- ex) AgentGPT looping between tester agent and programmer agent
